{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Racing Profile\n",
    "\n",
    "Thinking about all the features that would go into a single observation. \n",
    "Observation unit: driver\n",
    "Dependent variable: Grand Prix finishing position\n",
    "\n",
    "Independent variables:??\n",
    "\n",
    "    * Model 1: Features from Practice 1-3 and Qualifying\n",
    "    * Model 2: Model 1 + features from first half of race\n",
    "    * Model 3: Model 2 + features from past performance\n",
    "\n",
    "What Features?\n",
    "\n",
    "Model 1: Features from Practice 1-3 and Qualifying - Following features for each session (x4)\n",
    "\n",
    "    * Min/Max/Avg lap times\n",
    "    * Number of stints (stint_number max)\n",
    "    * Sum of # of practice laps?\n",
    "    * Summary stats for each lap? or stint? (break, rpm, throttle, speed, drs) (min/max/avg)\n",
    "        * max break, min/max/avg rpm, max/avg throttle, min/max/avg speed\n",
    "    * (Maybe) Number of gear changes (n_gear) \n",
    "    * Avg Pit duration\n",
    "    * Number of Pits?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: import\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "query_base = \"https://api.openf1.org/v1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "query_meetings = query_base+\"meetings?year=2023\"\n",
    "\n",
    "response = urlopen(query_meetings)\n",
    "data = json.loads(response.read().decode('utf-8'))\n",
    "meetings_df = pd.json_normalize(data)\n",
    "\n",
    "print(len(meetings_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must have equal len keys and value when setting with an ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(meeting)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# response = urlopen(query_sessions)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# data = json.loads(response.read().decode('utf-8'))\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# sessions_df = pd.json_normalize(data)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# sessions = list(sessions_df['session_key'])\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mmeeting_sessions_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmeeting_sessions_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmeeting_key\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmeeting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msessions_list\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp2\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp3\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(meeting_sessions_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/miniconda3/envs/dsan5300/lib/python3.13/site-packages/pandas/core/indexing.py:911\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 911\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dsan5300/lib/python3.13/site-packages/pandas/core/indexing.py:1890\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1885\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj[key] \u001b[38;5;241m=\u001b[39m infer_fill_value(value)\n\u001b[1;32m   1887\u001b[0m     new_indexer \u001b[38;5;241m=\u001b[39m convert_from_missing_indexer_tuple(\n\u001b[1;32m   1888\u001b[0m         indexer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m   1889\u001b[0m     )\n\u001b[0;32m-> 1890\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;66;03m# reindex the axis\u001b[39;00m\n\u001b[1;32m   1895\u001b[0m \u001b[38;5;66;03m# make sure to clear the cache because we are\u001b[39;00m\n\u001b[1;32m   1896\u001b[0m \u001b[38;5;66;03m# just replacing the block manager here\u001b[39;00m\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;66;03m# so the object is the same\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dsan5300/lib/python3.13/site-packages/pandas/core/indexing.py:1942\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[0;32m-> 1942\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1944\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[0;32m~/miniconda3/envs/dsan5300/lib/python3.13/site-packages/pandas/core/indexing.py:1982\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1977\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_with_indexer_frame_value(indexer, value, name)\n\u001b[1;32m   1979\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1980\u001b[0m     \u001b[38;5;66;03m# TODO: avoid np.ndim call in case it isn't an ndarray, since\u001b[39;00m\n\u001b[1;32m   1981\u001b[0m     \u001b[38;5;66;03m#  that will construct an ndarray, which will be wasteful\u001b[39;00m\n\u001b[0;32m-> 1982\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_2d_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1984\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ilocs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m lplane_indexer \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(pi):\n\u001b[1;32m   1985\u001b[0m     \u001b[38;5;66;03m# We are setting multiple rows in a single column.\u001b[39;00m\n\u001b[1;32m   1986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_column(ilocs[\u001b[38;5;241m0\u001b[39m], value, pi)\n",
      "File \u001b[0;32m~/miniconda3/envs/dsan5300/lib/python3.13/site-packages/pandas/core/indexing.py:2048\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_2d_value\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   2046\u001b[0m     value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m   2047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ilocs) \u001b[38;5;241m!=\u001b[39m value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m-> 2048\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2049\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust have equal len keys and value when setting with an ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2050\u001b[0m     )\n\u001b[1;32m   2052\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, loc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ilocs):\n\u001b[1;32m   2053\u001b[0m     value_col \u001b[38;5;241m=\u001b[39m value[:, i]\n",
      "\u001b[0;31mValueError\u001b[0m: Must have equal len keys and value when setting with an ndarray"
     ]
    }
   ],
   "source": [
    "meeting_list = meetings_df['meeting_key'].to_list()\n",
    "\n",
    "# print(meeting_list)\n",
    "\n",
    "for meeting in meeting_list:\n",
    "    query_sessions = query_base+\"sessions?meeting_key=\"+str(meeting)\n",
    "    print(meeting)\n",
    "\n",
    "    # response = urlopen(query_sessions)\n",
    "    # data = json.loads(response.read().decode('utf-8'))\n",
    "    # sessions_df = pd.json_normalize(data)\n",
    "\n",
    "    # sessions = list(sessions_df['session_key'])\n",
    "\n",
    "    meeting_sessions_df.loc[meeting_sessions_df['meeting_key'] == meeting, 'sessions_list'] = [['p1','p2','p3','q','r']]\n",
    "\n",
    "\n",
    "print(meeting_sessions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9465, 9466, 9467, 9468, 9472]\n"
     ]
    }
   ],
   "source": [
    "# get session numbers for practice and qualifiying, preloaded meeting_number for now\n",
    "query_sessions = query_base+\"sessions?meeting_key=1229\"\n",
    "\n",
    "response = urlopen(query_sessions)\n",
    "data = json.loads(response.read().decode('utf-8'))\n",
    "sessions_df = pd.json_normalize(data)\n",
    "\n",
    "# get just the session keys \n",
    "sessions = list(sessions_df['session_key'])\n",
    "# remove the real race\n",
    "del sessions[-1]\n",
    "print(sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m driver_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# create features data frame\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# create list of features for a individual driver\u001b[39;00m\n\u001b[1;32m      8\u001b[0m driver_feats \u001b[38;5;241m=\u001b[39m [driver_number]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# I'll just do one driver for now\n",
    "driver_number = str(16)\n",
    "\n",
    "# create features data frame\n",
    "X = pd.DataFrame()\n",
    "\n",
    "# create list of features for a individual driver\n",
    "driver_feats = [driver_number]\n",
    "\n",
    "session_sub = sessions[0]\n",
    "\n",
    "\n",
    "for session in session_sub:\n",
    "    # creat list to store all data for this session\n",
    "    session_feats = []\n",
    "\n",
    "    # LAPS QUERY\n",
    "    query_laps = query_base+\"laps?driver_number=\"+driver_number+\"&session_key=\"+str(session)\n",
    "\n",
    "    # call api and convert to df\n",
    "    response = urlopen(query_laps)\n",
    "    data = json.loads(response.read().decode('utf-8'))\n",
    "    laps_df = pd.json_normalize(data)\n",
    "\n",
    "    # extract lap infor for current session\n",
    "    min_lap = laps_df['lap_duration'].min()\n",
    "    max_lap = laps_df['lap_duration'].max()\n",
    "    avg_lap = float(round(laps_df['lap_duration'].mean(),3))\n",
    "    num_laps = laps_df['lap_number'].max()\n",
    "\n",
    "    # PARSE THE LAPS DATA BY TIME\n",
    "    # laps_times_df will be used for car_data queries\n",
    "    lap_times = laps_df[['lap_number','date_start','lap_duration']]\n",
    "\n",
    "    # Convert date_start to datetime if it's not already in datetime format\n",
    "    lap_times['date_start'] = pd.to_datetime(lap_times['date_start'])\n",
    "\n",
    "    # use the next lap start as the end time exept for the last lap, which will be calculated with lap duration\n",
    "    lap_times['date_end'] = lap_times['date_start'].shift(-1).fillna(lap_times['date_start'] + pd.to_timedelta(lap_times['lap_duration'], unit='s'))\n",
    "\n",
    "    # convert back to string\n",
    "    lap_times['date_start'] = lap_times['date_start'].dt.strftime('%Y-%m-%dT%H:%M:%S.%f') + lap_times['date_start'].dt.strftime('%z').str[:3] + ':' + lap_times['date_start'].dt.strftime('%z').str[3:]\n",
    "    lap_times['date_end'] = lap_times['date_end'].dt.strftime('%Y-%m-%dT%H:%M:%S.%f') + lap_times['date_end'].dt.strftime('%z').str[:3] + ':' + lap_times['date_end'].dt.strftime('%z').str[3:]\n",
    "\n",
    "    # find the lap number for their best lap\n",
    "    min_lap_num = laps_df[laps_df['lap_duration']==min_lap]['lap_number'].to_list()[0]\n",
    "\n",
    "\n",
    "    # CONDUCT A CAR DATA QUERY ON THE MINIMUM LAP\n",
    "    # base car data query for time filter to be added to\n",
    "    query_car_base = query_base+\"car_data?driver_number=\"+driver_number+\"&session_key=\"+str(session)\n",
    "\n",
    "    # create staret and end time for car data query\n",
    "    start_time = lap_times[lap_times['lap_number']==min_lap_num]['date_start'].to_list()[0]\n",
    "    end_time = lap_times[lap_times['lap_number']==min_lap_num]['date_end'].to_list()[0]\n",
    "\n",
    "    # query for lap specific times\n",
    "    query_car = query_car_base + \"&date>=\"+str(start_time)+\"&date<=\"+str(end_time)\n",
    "\n",
    "    # call api for car data with lap time query\n",
    "    response = urlopen(query_car)\n",
    "    data = json.loads(response.read().decode('utf-8'))\n",
    "    car_df = pd.json_normalize(data)\n",
    "\n",
    "    # get summary stats for the lap\n",
    "    max_break = car_df['brake'].max()\n",
    "    max_rpm = car_df['rpm'].max()\n",
    "    min_rpm = car_df['rpm'].min()\n",
    "    avg_rpm = round(car_df['rpm'].mean())\n",
    "    max_throttle = car_df['throttle'].max()\n",
    "    avg_throttle = round(car_df['throttle'].mean())\n",
    "    min_speed = car_df['speed'].min()\n",
    "    max_speed = car_df['speed'].max()\n",
    "    avg_speed = round(car_df['speed'].mean())\n",
    "\n",
    "    # create list of car_data stats per lap\n",
    "    min_lap_stats = [max_break, min_rpm, max_rpm, avg_rpm, max_throttle, avg_throttle, min_speed, max_speed, avg_speed]\n",
    "\n",
    "\n",
    "    # STINTS QUERY FOR NUMBER OF STINTS\n",
    "    query_stints = query_base + \"stints?driver_number=\"+driver_number+\"&session_key=\"+str(session)\n",
    "\n",
    "    # call api and convert to df\n",
    "    response = urlopen(query_stints)\n",
    "    data = json.loads(response.read().decode('utf-8'))\n",
    "    stints_df = pd.json_normalize(data)\n",
    "\n",
    "    # extract max stint number\n",
    "    num_stints = stints_df['stint_number'].max()\n",
    "\n",
    "\n",
    "    # PTIS QUERY\n",
    "    query_pits = query_base + \"pit?driver_number=\"+driver_number+\"&session_key=\"+str(session)\n",
    "\n",
    "    # call api and convert to df\n",
    "    response = urlopen(query_pits)\n",
    "    data = json.loads(response.read().decode('utf-8'))\n",
    "    pits_df = pd.json_normalize(data)\n",
    "\n",
    "    # extract num of pits and avg pit duration\n",
    "    num_pits = len(pits_df)\n",
    "    avg_pit_time = float(round(pits_df['pit_duration'].mean(),1))\n",
    "\n",
    "    # WEATHER QUERY\n",
    "    query_wx = query_base + \"weather?&session_key=\"+str(session)\n",
    "\n",
    "    # call api and convert to df\n",
    "    response = urlopen(query_wx)\n",
    "    data = json.loads(response.read().decode('utf-8'))\n",
    "    weather_df = pd.json_normalize(data)\n",
    "\n",
    "    ### parse weather data\n",
    "\n",
    "    # max_rain\n",
    "    # max_wind\n",
    "    # avg_air_temp\n",
    "    # avg_track_temp\n",
    "\n",
    "    wx_stats = ['max_rain', 'max_wind', 'avg_air_temp', 'avg_track_temp']\n",
    "\n",
    "\n",
    "    # RADIO QUERY\n",
    "    query_radio = query_base + \"team_radio?driver_number=\"+driver_number+\"&session_key=\"+str(session)\n",
    "\n",
    "    # call api and convert to df\n",
    "    response = urlopen(query_radio)\n",
    "    data = json.loads(response.read().decode('utf-8'))\n",
    "    radio_df = pd.json_normalize(data)\n",
    "\n",
    "    # numer of radio calls during the session\n",
    "    num_radios = len(radio_df)\n",
    "\n",
    "\n",
    "    # AFTER ALL QUERIES PER SESSION\n",
    "    # append to driver features list\n",
    "    session_feats = session_feats + [min_lap, max_lap, avg_lap, num_laps, num_stints] + [num_pits, avg_pit_time] + min_lap_stats + wx_stats + num_radios\n",
    "\n",
    "\n",
    "print(driver_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "print(len(driver_feats))\n",
    "\n",
    "# colnames = ['min_lap_p1', 'max_lap_p1', 'avg_lap_p1', 'num_laps_p1, 'num_stints_p1', 'num_pits_p1', 'avg_pit_time_p1',\n",
    "#            'max_break_p1', 'min_rpm_p1', 'max_rpm_p1', 'avg_rpm_p1', 'max_throttle_p1', 'avg_throttle_p1', 'min_speed_p1',\n",
    "#            'max_speed_p1', 'avg_speed_p1', 'max_rain_p1', 'max_wind_p1', 'avg_air_temp_p1', 'avg_track_temp_p1', 'num_radios_p1',\n",
    "#            'min_lap_p2', 'max_lap_p2', 'avg_lap_p2', 'num_laps_p2, 'num_stints_p2', 'num_pits_p2', 'avg_pit_time_p2',\n",
    "#            'max_break_p2', 'min_rpm_p2', 'max_rpm_p2', 'avg_rpm_p2', 'max_throttle_p2', 'avg_throttle_p2', 'min_speed_p2',\n",
    "#            'max_speed_p2', 'avg_speed_p2', 'max_rain_p2', 'max_wind_p2', 'avg_air_temp_p2', 'avg_track_temp_p2', 'num_radios_p2',\n",
    "#            'min_lap_p3', 'max_lap_p3', 'avg_lap_p3', 'num_laps_p3, 'num_stints_p3', 'num_pits_p3', 'avg_pit_time_p3',\n",
    "#            'max_break_p3', 'min_rpm_p3', 'max_rpm_p3', 'avg_rpm_p3', 'max_throttle_p3', 'avg_throttle_p3', 'min_speed_p3',\n",
    "#            'max_speed_p3', 'avg_speed_p3','max_rain_p3', 'max_wind_p3', 'avg_air_temp_p3', 'avg_track_temp_p3', 'num_radios_p3',\n",
    "#            'min_lap_q', 'max_lap_q', 'avg_lap_q', 'num_laps_q, 'num_stints_q','num_pits_q', 'avg_pit_time_q',\n",
    "#            'max_break_q', 'min_rpm_q', 'max_rpm_q', 'avg_rpm_q', 'max_throttle_q', 'avg_throttle_q', 'min_speed_q', \n",
    "#            'max_speed_q', 'avg_speed_q','max_rain_q', 'max_wind_q', 'avg_air_temp_q', 'avg_track_temp_q', 'num_radios_q',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at car data for individual laps\n",
    "* max break, min/max/avg rpm, max/avg throttle, min/max/avg speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2758\n",
      "[104, 0, 12084, 2014, 104, 79, 0, 276, 26]\n"
     ]
    }
   ],
   "source": [
    "lap_num = 7\n",
    "\n",
    "# base car data query for time filter to be added to\n",
    "query_car_base = query_base+\"car_data?driver_number=\"+driver_number+\"&session_key=\"+str(session)\n",
    "\n",
    "# create staret and end time for car data query\n",
    "start_time = lap_times[lap_times['lap_number']==lap_num]['date_start'].to_list()[0]\n",
    "end_time = lap_times[lap_times['lap_number']==lap_num]['date_end'].to_list()[0]\n",
    "\n",
    "# query for lap specific times\n",
    "query_car = query_car_base + \"&date>=\"+str(start_time)+\"&date<=\"+str(end_time)\n",
    "\n",
    "# call api for car data with lap time query\n",
    "response = urlopen(query_car)\n",
    "data = json.loads(response.read().decode('utf-8'))\n",
    "car_df = pd.json_normalize(data)\n",
    "print(len(car_df))\n",
    "\n",
    "max_break = car_df['brake'].max()\n",
    "max_rpm = car_df['rpm'].max()\n",
    "min_rpm = car_df['rpm'].min()\n",
    "avg_rpm = round(car_df['rpm'].mean())\n",
    "max_throttle = car_df['throttle'].max()\n",
    "avg_throttle = round(car_df['throttle'].mean())\n",
    "min_speed = car_df['speed'].min()\n",
    "max_speed = car_df['speed'].max()\n",
    "avg_speed = round(car_df['speed'].mean())\n",
    "\n",
    "car_data_list = [max_break, min_rpm, max_rpm, avg_rpm, max_throttle, avg_throttle, min_speed, max_speed, avg_speed]\n",
    "\n",
    "print(car_data_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan5300",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
